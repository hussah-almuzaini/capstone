<!-- <!DOCTYPE html>
<html lang="ar">
<head>
  <meta charset="UTF-8" />
  <title>Ù…Ø³Ø§Ø¹Ø¯ Ø§Ù„Ù…ÙƒÙÙˆÙÙŠÙ†</title>
  <link href="https://fonts.googleapis.com/css2?family=Tajawal:wght@400;700&display=swap" rel="stylesheet">
  <link rel="stylesheet" href="/static/style.css" />
  <link rel="icon" href="/static/favicon.ico" type="image/x-icon">
</head>
<body>
  <img id="logo" src="/static/logo.png" alt="Ø´Ø¹Ø§Ø± Ø¨ØµÙŠØ±">
  <h1>ğŸ” Ù…Ø³Ø§Ø¹Ø¯ Ø§Ù„Ù…ÙƒÙÙˆÙÙŠÙ† - Ø§Ù„Ø¨Ø­Ø« Ø§Ù„ØµÙˆØªÙŠ</h1>
  <p id="heardText">ğŸ§ ÙÙŠ Ø§Ù†ØªØ¸Ø§Ø± Ø§Ù„Ø£Ù…Ø± Ø§Ù„ØµÙˆØªÙŠ...</p>
  <pre id="result"></pre>
  <audio id="audio" controls style="display:none"></audio>
  <video id="video" width="320" height="240" autoplay muted></video>

  <script>
    let ws;
    let detectionTarget = "person";
    function playAudioBlob(blob, callback = null) {
      const audio = document.getElementById("audio");
      const url = URL.createObjectURL(blob);

      // Stop current playback
      audio.pause();
      audio.currentTime = 0;

      // Replace source and play
      audio.src = url;
      audio.style.display = 'block';

      // Play and wait until it ends before allowing next
      audio.play().then(() => {
        audio.onended = () => {
          if (callback) callback();
        };
      }).catch(e => {
        console.warn("Audio error:", e.message);
        if (callback) callback();  // Fallback even on error
      });
    }
    function speak(text, callback = null) {
      const formData = new FormData();
      formData.append("text", text);
      fetch("/speak", { method: "POST", body: formData })
        .then(response => response.blob())
        .then(blob => {
          playAudioBlob(blob, callback);
        });
    }

    function listenForCommandOrTarget(callback) {
      const r = new (window.SpeechRecognition || window.webkitSpeechRecognition)();
      r.lang = 'ar-SA';
      r.onresult = e => {
        const t = e.results[0][0].transcript.trim().toLowerCase();
        document.getElementById("heardText").innerText = `ğŸ’¬ Ø³Ù…Ø¹: ${t}`;

        if (t.includes("Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©") || t.includes("Ø±Ø¬ÙˆØ¹") || t.includes("Ù‡ÙˆÙ…")|| t.includes("Ø§Ø±Ø¬Ø¹")|| t.includes("Ø§Ø®Ø±Ø¬")) {
          detectionTarget = "back";
          speak("Ø¬Ø§Ø±ÙŠ Ø§Ù„Ø±Ø¬ÙˆØ¹ Ø¥Ù„Ù‰ Ø§Ù„ØµÙØ­Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©");
          window.location.href = "/";
          return;
        }

        if (t.includes("Ø´Ø®Øµ") || t.includes("1")) {
          detectionTarget = "person";
        } else if (t.includes("ÙƒØ±Ø³ÙŠ") || t.includes("2")) {
          detectionTarget = "chair";
        } else if (t.includes("Ù‡Ø§ØªÙ") || t.includes("3")) {
          detectionTarget = "cell phone";
        } else if (t.includes("Ø­Ù‚ÙŠØ¨Ø©") || t.includes("4")) {
          detectionTarget = "handbag";
        } else if (t.includes("Ø·Ø§ÙˆÙ„Ø©") || t.includes("5")) {
          detectionTarget = "dining table";
        } else if (t.includes("ØªÙ„ÙØ§Ø²") || t.includes("6")) {
          detectionTarget = "tv";
        }
          else {
          speak("Ù„Ù… Ø£ÙÙ‡Ù… Ø§Ù„Ø£Ù…Ø±. Ø­Ø§ÙˆÙ„ Ù…Ø±Ø© Ø£Ø®Ø±Ù‰.");
          return;
        }

        if (callback) callback();
      };
      r.start();
    }

    async function startDetection() {
      ws = new WebSocket(`ws://${location.host}/ws/depth-live`);
      ws.onopen = () => {
        ws.send(JSON.stringify({ target: detectionTarget }));
      };
      ws.onmessage = (event) => {
        const data = JSON.parse(event.data);
        document.getElementById('result').textContent = data.description || data.error || '';
        if (data.audio_url) {
          const audio = document.getElementById('audio');
          fetch(data.audio_url)
            .then(response => response.blob())
            .then(blob => {
              playAudioBlob(blob, () => {
                speak("Ù‡Ù„ ØªØ±ÙŠØ¯ Ø§Ù„ÙƒØ´Ù Ø¹Ù† Ù‡Ø¯Ù Ø¢Ø®Ø± Ø£Ùˆ Ø§Ù„Ø±Ø¬ÙˆØ¹ØŸ", () => {
                  listenForCommandOrTarget(() => {
                    if (detectionTarget === "back") {
                      speak("Ø¬Ø§Ø±ÙŠ Ø§Ù„Ø±Ø¬ÙˆØ¹ Ø¥Ù„Ù‰ Ø§Ù„ØµÙØ­Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©");
                      window.location.href = "/";
                    } else {
                      ws.send(JSON.stringify({ target: detectionTarget }));
                    }
                  });
                });
              });
            });
          // audio.onended = () => {
          //   speak("Ù‡Ù„ ØªØ±ÙŠØ¯ Ø§Ù„ÙƒØ´Ù Ø¹Ù† Ù‡Ø¯Ù Ø¢Ø®Ø± Ø£Ùˆ Ø§Ù„Ø±Ø¬ÙˆØ¹ØŸ", () => {
          //     listenForCommandOrTarget(() => {
          //       ws.send(JSON.stringify({ target: detectionTarget }));
          //     });
          //   });
          // };
        }
      };

      const video = document.getElementById('video');
      const stream = await navigator.mediaDevices.getUserMedia({ video: true });
      video.srcObject = stream;

      const canvas = document.createElement('canvas');
      const ctx = canvas.getContext('2d');

      setInterval(() => {
        canvas.width = video.videoWidth;
        canvas.height = video.videoHeight;
        ctx.drawImage(video, 0, 0);
        canvas.toBlob(blob => {
          if (ws.readyState === 1) {
            const reader = new FileReader();
            reader.onload = () => {
              ws.send(reader.result);
            };
            reader.readAsDataURL(blob);
          }
        }, 'image/jpeg', 0.7);
      }, 300);
    }

    window.onload = () => {
      speak("Ø§Ø®ØªØ± Ø§Ù„Ù‡Ø¯Ù: 1 Ø´Ø®ØµØŒ 2 ÙƒØ±Ø³ÙŠØŒ 3 Ù‡Ø§ØªÙØŒ 4 Ø­Ù‚ÙŠØ¨Ø©ØŒ 5 Ø·Ø§ÙˆÙ„Ø©ØŒ 6 ØªÙ„ÙØ§Ø²", () => {
        listenForCommandOrTarget(() => {
          startDetection();
        });
      });
    };
  </script>
</body>
</html> -->
<!-- old version works like a charm -->



<!-- new version should be better -->
<!-- 
<!DOCTYPE html>
<html lang="ar">
<head>
  <meta charset="UTF-8" />
  <title>Ù…Ø³Ø§Ø¹Ø¯ Ø§Ù„Ù…ÙƒÙÙˆÙÙŠÙ†</title>
  <link href="https://fonts.googleapis.com/css2?family=Tajawal:wght@400;700&display=swap" rel="stylesheet">
  <link rel="stylesheet" href="/static/style.css" />
  <link rel="icon" href="/static/favicon.ico" type="image/x-icon">
</head>
<body>
  <img id="logo" src="/static/logo.png" alt="Ø´Ø¹Ø§Ø± Ø¨ØµÙŠØ±">
  <h1>ğŸ” Ù…Ø³Ø§Ø¹Ø¯ Ø§Ù„Ù…ÙƒÙÙˆÙÙŠÙ† - Ø§Ù„Ø¨Ø­Ø« Ø§Ù„ØµÙˆØªÙŠ</h1>
  <p id="heardText">ğŸ§ ÙÙŠ Ø§Ù†ØªØ¸Ø§Ø± Ø§Ù„Ø£Ù…Ø± Ø§Ù„ØµÙˆØªÙŠ...</p>
  <pre id="result"></pre>
  <audio id="audio" controls style="display:none"></audio>
  <video id="video" width="320" height="240" autoplay muted></video>

  <script>
    let ws;
    let detectionTarget = "person";
    let detectionCooldown = {};
    let lastPromptTime = 0;
    const audio = document.getElementById("audio");

    function speak(text, callback = null) {
      const formData = new FormData();
      formData.append("text", text);
      fetch("/speak", { method: "POST", body: formData })
        .then(response => response.blob())
        .then(blob => {
          const url = URL.createObjectURL(blob);
          audio.pause();
          audio.currentTime = 0;
          audio.src = url;
          audio.style.display = 'block';
          audio.play();
          if (callback) audio.onended = callback;
        });
    }

    function listenForExitCommand() {
      const r = new (window.SpeechRecognition || window.webkitSpeechRecognition)();
      r.lang = 'ar-SA';
      r.onresult = e => {
        const t = e.results[0][0].transcript.trim().toLowerCase();
        if (t.includes("Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©") || t.includes("Ø±Ø¬ÙˆØ¹") || t.includes("ØªÙˆÙ‚Ù")) {
          speak("Ø¬Ø§Ø±ÙŠ Ø§Ù„Ø±Ø¬ÙˆØ¹ Ø¥Ù„Ù‰ Ø§Ù„ØµÙØ­Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©", () => {
            window.location.href = "/";
          });
        }
      };
      r.start();
    }

    async function startDetection() {
      ws = new WebSocket(`ws://${location.host}/ws/depth-live`);
      ws.onopen = () => {
        ws.send(JSON.stringify({ target: detectionTarget }));
      };

      ws.onmessage = (event) => {
        const data = JSON.parse(event.data);
        if (data.description) {
          const now = Date.now();
          // if (!detectionCooldown[data.description] || (now - detectionCooldown[data.description] > 7000)) {
          //   detectionCooldown[data.description] = now;
          document.getElementById('result').textContent = data.description;

          if (data.audio_url) {
            fetch(data.audio_url)
              .then(response => response.blob())
              .then(blob => {
                const url = URL.createObjectURL(blob);
                audio.pause();
                audio.currentTime = 0;
                audio.src = url;
                audio.style.display = 'block';
                audio.play();
              });
          }
          // }
        }
      };

      const video = document.getElementById('video');
      const stream = await navigator.mediaDevices.getUserMedia({ video: true });
      video.srcObject = stream;

      const canvas = document.createElement('canvas');
      const ctx = canvas.getContext('2d');

      setInterval(() => {
        canvas.width = video.videoWidth;
        canvas.height = video.videoHeight;
        ctx.drawImage(video, 0, 0);
        canvas.toBlob(blob => {
          if (ws.readyState === 1) {
            const reader = new FileReader();
            reader.onload = () => {
              ws.send(reader.result);
            };
            reader.readAsDataURL(blob);
          }
        }, 'image/jpeg', 0.7);
      }, 500);
    }

    window.onload = () => {
      speak("Ù…Ø±Ø­Ø¨Ù‹Ø§ØŒ Ø§Ø®ØªØ± Ø§Ù„Ù‡Ø¯Ù Ø§Ù„Ù…Ø·Ù„ÙˆØ¨: 1 Ø´Ø®ØµØŒ 2 ÙƒØ±Ø³ÙŠØŒ 3 ÙƒÙ…Ø¨ÙŠÙˆØªØ±ØŒ 4 Ø­Ù‚ÙŠØ¨Ø©ØŒ 5 Ø·Ø§ÙˆÙ„Ø©ØŒ 6 ØªÙ„ÙØ§Ø². Ø£Ùˆ Ù‚Ù„ Ø±Ø¬ÙˆØ¹ Ù„Ù„Ø¹ÙˆØ¯Ø© Ø¥Ù„Ù‰ Ø§Ù„ØµÙØ­Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©.", () => {
        const r = new (window.SpeechRecognition || window.webkitSpeechRecognition)();
        r.lang = 'ar-SA';
        r.onresult = e => {
          const t = e.results[0][0].transcript.trim().toLowerCase();
          if (t.includes("Ø´Ø®Øµ") || t.includes("1")) detectionTarget = "person";
          else if (t.includes("ÙƒØ±Ø³ÙŠ") || t.includes("2")) detectionTarget = "chair";
          else if (t.includes("ÙƒÙ…Ø¨ÙŠÙˆØªØ±") || t.includes("3")) detectionTarget = "laptop";
          else if (t.includes("Ø­Ù‚ÙŠØ¨Ø©") || t.includes("4")) detectionTarget = "backpack";
          else if (t.includes("Ø·Ø§ÙˆÙ„Ø©") || t.includes("5")) detectionTarget = "dining table";
          else if (t.includes("ØªÙ„ÙØ§Ø²") || t.includes("6")) detectionTarget = "tv";
          else if (t.includes("Ø±Ø¬ÙˆØ¹") || t.includes("Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©")) {
            window.location.href = "/";
            return;
          }
          startDetection();
          // Start passive exit listening every 15 seconds
          setInterval(listenForExitCommand, 15000);
        };
        r.start();
      });
    };
  </script>
</body>
</html> -->



<!-- extra version -->

<!-- <!DOCTYPE html>
<html lang="ar">
<head>
  <meta charset="UTF-8" />
  <title>Ù…Ø³Ø§Ø¹Ø¯ Ø§Ù„Ù…ÙƒÙÙˆÙÙŠÙ†</title>
  <link href="https://fonts.googleapis.com/css2?family=Tajawal:wght@400;700&display=swap" rel="stylesheet">
  <link rel="stylesheet" href="/static/style.css" />
  <link rel="icon" href="/static/favicon.ico" type="image/x-icon">
</head>
<body>
  <img id="logo" src="/static/logo.png" alt="Ø´Ø¹Ø§Ø± Ø¨ØµÙŠØ±">
  <h1>ğŸ” Ù…Ø³Ø§Ø¹Ø¯ Ø§Ù„Ù…ÙƒÙÙˆÙÙŠÙ† - Ø§Ù„Ø¨Ø­Ø« Ø§Ù„ØµÙˆØªÙŠ</h1>
  <p id="heardText">ğŸ§ ÙÙŠ Ø§Ù†ØªØ¸Ø§Ø± Ø§Ù„Ø£Ù…Ø± Ø§Ù„ØµÙˆØªÙŠ...</p>
  <pre id="result"></pre>
  <audio id="audio" controls style="display:none"></audio>
  <video id="video" width="640" height="440" autoplay muted></video>

  <script>
    let ws;
    let detectionTarget = "person";
    const audio = document.getElementById("audio");
    let isListening = false;
    let lastDescription = "";

    function speak(text, callback = null) {
      const formData = new FormData();
      formData.append("text", text);
      fetch("/speak", { method: "POST", body: formData })
        .then(response => response.blob())
        .then(blob => {
          const url = URL.createObjectURL(blob);
          audio.pause();
          audio.currentTime = 0;
          audio.src = url;
          audio.style.display = 'block';

          audio.play().then(() => {
            console.log("ğŸ”Š Audio playing...");
          }).catch(err => {
            console.warn("âŒ Audio play failed:", err.message);
          });

          audio.onended = () => {
            console.log("âœ… Audio ended");
            isListening = false;
            if (callback) callback();
          };

          audio.onerror = () => {
            console.warn("âŒ Audio playback error");
            isListening = false;
            if (callback) callback();
          };
        });
    }

    function continuousExitListener() {
      if (isListening || !audio.paused || userCommandInProgress) return;

      isListening = false;
      userCommandInProgress = false;

      speak("Ø£Ù†Ø§ Ø£Ø³ØªÙ…Ø¹ Ø§Ù„Ø¢Ù†. Ù‚Ù„ Ø±Ø¬ÙˆØ¹ Ù„Ù„Ø¹ÙˆØ¯Ø© Ø¥Ù„Ù‰ Ø§Ù„ØµÙØ­Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©.", () => {
        try {
          const r = new (window.SpeechRecognition || window.webkitSpeechRecognition)();
          r.lang = 'ar-SA';

          r.onresult = e => {
            const t = e.results[0][0].transcript.trim().toLowerCase();
            if (t.includes("Ø±Ø¬ÙˆØ¹") || t.includes("Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©") || t.includes("ØªÙˆÙ‚Ù")) {
              speak("Ø¬Ø§Ø±ÙŠ Ø§Ù„Ø±Ø¬ÙˆØ¹ Ø¥Ù„Ù‰ Ø§Ù„ØµÙØ­Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©", () => {
                window.location.href = "/";
              });
            } else {
              userCommandInProgress = false;
            }
          };

          r.onend = () => {
            isListening = false;
            userCommandInProgress = false;
          };
          r.onerror = () => {
            isListening = false;
            userCommandInProgress = false;
          };

          r.start();
        } catch (err) {
          console.error("SpeechRecognition error:", err);
          isListening = false;
          userCommandInProgress = false;
        }
      });
    }


    async function startDetection() {
      ws = new WebSocket(`ws://${location.host}/ws/depth-live`);

      ws.onopen = () => {
        ws.send(JSON.stringify({ target: detectionTarget }));
        console.log("âœ… WebSocket opened");
      };

      ws.onmessage = (event) => {
        const data = JSON.parse(event.data);
        console.log("ğŸ“¦ WS Message Received:", JSON.stringify(data));

        if (!data.description) return;
        if (data.description === lastDescription) return;  // skip duplicates
        lastDescription = data.description;

        document.getElementById('result').textContent = data.description;

        if (data.audio_url) {
          fetch(data.audio_url)
            .then(response => response.blob())
            .then(blob => {
              const url = URL.createObjectURL(blob);
              if (audio.src !== url) {
                audio.pause();
                audio.currentTime = 0;
                audio.src = url;
              }
              audio.style.display = 'block';
              audio.play().then(() => {
                console.log("ğŸ”Š Audio playing...");
              }).catch(err => {
                console.warn("âŒ Audio play failed:", err.message);
              });
            });
        }
      };

      ws.onclose = () => {
        console.warn("âŒ WebSocket closed");
        speak("ØªÙ… ÙÙ‚Ø¯ Ø§Ù„Ø§ØªØµØ§Ù„ Ø¨Ø§Ù„Ø®Ø§Ø¯Ù…. ÙŠØ±Ø¬Ù‰ Ø¥Ø¹Ø§Ø¯Ø© ØªØ­Ù…ÙŠÙ„ Ø§Ù„ØµÙØ­Ø©.");
      };

      const video = document.getElementById('video');
      const stream = await navigator.mediaDevices.getUserMedia({ video: true });
      video.srcObject = stream;

      const canvas = document.createElement('canvas');
      const ctx = canvas.getContext('2d');

      setInterval(() => {
        canvas.width = video.videoWidth;
        canvas.height = video.videoHeight;
        ctx.drawImage(video, 0, 0);
        canvas.toBlob(blob => {
          if (ws.readyState === 1) {
            const reader = new FileReader();
            reader.onload = () => {
              ws.send(reader.result);
            };
            reader.readAsDataURL(blob);
          }
        }, 'image/jpeg', 0.7);
      }, 500);
    }

    window.onload = () => {
      speak("Ù…Ø±Ø­Ø¨Ù‹Ø§ØŒ Ø§Ø®ØªØ± Ø§Ù„Ù‡Ø¯Ù Ø§Ù„Ù…Ø·Ù„ÙˆØ¨: 1 Ø´Ø®ØµØŒ 2 ÙƒØ±Ø³ÙŠØŒ 3 ÙƒÙ…Ø¨ÙŠÙˆØªØ±ØŒ 4 Ø­Ù‚ÙŠØ¨Ø©ØŒ 5 Ø·Ø§ÙˆÙ„Ø©ØŒ 6 ØªÙ„ÙØ§Ø²ØŒ 7 Ù‡Ø§ØªÙ. Ø£Ùˆ Ù‚Ù„ Ø±Ø¬ÙˆØ¹ Ù„Ù„Ø¹ÙˆØ¯Ø© Ø¥Ù„Ù‰ Ø§Ù„ØµÙØ­Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©.", () => {
        const r = new (window.SpeechRecognition || window.webkitSpeechRecognition)();
        r.lang = 'ar-SA';

        r.onresult = e => {
          const t = e.results[0][0].transcript.trim().toLowerCase();
          if (t.includes("Ø´Ø®Øµ") || t.includes("1")) detectionTarget = "person";
          else if (t.includes("ÙƒØ±Ø³ÙŠ") || t.includes("2")) detectionTarget = "chair";
          else if (t.includes("ÙƒÙ…Ø¨ÙŠÙˆØªØ±") || t.includes("3")) detectionTarget = "laptop";
          else if (t.includes("Ø­Ù‚ÙŠØ¨Ø©") || t.includes("4")) detectionTarget = "backpack";
          else if (t.includes("Ø·Ø§ÙˆÙ„Ø©") || t.includes("5")) detectionTarget = "dining table";
          else if (t.includes("ØªÙ„ÙØ§Ø²") || t.includes("6")) detectionTarget = "tv";
          else if (t.includes("Ù‡Ø§ØªÙ") || t.includes("7")) detectionTarget = "cell phone";
          else if (t.includes("Ø±Ø¬ÙˆØ¹") || t.includes("Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©")) {
            window.location.href = "/";
            return;
          }

          startDetection();
          setInterval(continuousExitListener, 30000);
        };

        r.onerror = () => console.warn("ğŸ™ï¸ Initial SpeechRecognition error");
        r.start();
      });
    };
  </script>
</body>
</html> -->


<!-- final version -->
<!DOCTYPE html>
<html lang="ar">
<head>
  <meta charset="UTF-8" />
  <title>Ù…Ø³Ø§Ø¹Ø¯ Ø§Ù„Ù…ÙƒÙÙˆÙÙŠÙ†</title>
  <link href="https://fonts.googleapis.com/css2?family=Tajawal:wght@400;700&display=swap" rel="stylesheet">
  <link rel="stylesheet" href="/static/style.css" />
  <link rel="icon" href="/static/favicon.ico" type="image/x-icon">
</head>
<div class="corner-button" id="corner-top-left" onclick="window.location.href='/'"></div>
<style>
  .corner-button {
    position: fixed;
    width: 20vw;
    height: 20vh;
    z-index: 999;
    background: transparent;
  }
  #corner-top-left {
    top: 0;
    left: 0;
  }
</style>
<body>
  <img id="logo" src="/static/logo.png" alt="Ø´Ø¹Ø§Ø± Ø¨ØµÙŠØ±">
  <h1>ğŸ” Ù…Ø³Ø§Ø¹Ø¯ Ø§Ù„Ù…ÙƒÙÙˆÙÙŠÙ† - Ø§Ù„Ø¨Ø­Ø« Ø§Ù„ØµÙˆØªÙŠ</h1>
  <p id="heardText">ğŸ§ ÙÙŠ Ø§Ù†ØªØ¸Ø§Ø± Ø§Ù„Ø£Ù…Ø± Ø§Ù„ØµÙˆØªÙŠ...</p>
  <pre id="result"></pre>
  <audio id="audio" controls style="display:none"></audio>
  <audio id="beep" src="/static/beep.wav" preload="auto"></audio>
  <video
    id="video"
    width="640"
    height="480"
    autoplay
    muted
    style="display: block; margin: 0 auto; background-color: #000; border: 2px solid #ccc;"
  ></video>

  <script>
    let ws;
    let detectionTarget = "person";
    const audio = document.getElementById("audio");
    const beep = document.getElementById("beep");
    let isListening = false;
    let userCommandInProgress = false;
    let lastDescription = "";
    let detectionCount = 0;
    let detectionLocked = false;
    let timeoutHandle = null;

    function speak(text, callback = null) {
      const formData = new FormData();
      formData.append("text", text);
      fetch("/speak", { method: "POST", body: formData })
        .then(response => response.blob())
        .then(blob => {
          const url = URL.createObjectURL(blob);
          audio.pause();
          audio.currentTime = 0;
          audio.src = url;
          audio.style.display = 'block';

          audio.play().then(() => {
            console.log("ğŸ”Š Audio playing...");
          }).catch(err => {
            console.warn("âŒ Audio play failed:", err.message);
          });

          audio.onended = () => {
            console.log("âœ… Audio ended");
            isListening = false;
            if (callback) callback();
          };

          audio.onerror = () => {
            console.warn("âŒ Audio playback error");
            isListening = false;
            if (callback) callback();
          };
        });
    }

    function stopDetectionStream() {
      if (ws && ws.readyState === 1) {
        ws.close();
        console.log("ğŸ›‘ Detection stream stopped.");
      }
    }

    function askAfterDetections() {
      detectionLocked = true;
      stopDetectionStream();

      speak("Ù‡Ù„ ØªØ±ÙŠØ¯ Ø§Ù„Ø§Ø³ØªÙ…Ø±Ø§Ø± ÙÙŠ Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ù†ÙØ³ Ø§Ù„Ù‡Ø¯ÙØŒ Ø£Ùˆ Ø§Ø®ØªÙŠØ§Ø± Ù‡Ø¯Ù Ø¢Ø®Ø±ØŒ Ø£Ùˆ Ø§Ù„Ø±Ø¬ÙˆØ¹ Ø¥Ù„Ù‰ Ø§Ù„ØµÙØ­Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©ØŸ", () => {
        const r = new (window.SpeechRecognition || window.webkitSpeechRecognition)();
        r.lang = 'ar-SA';

        const cleanup = () => {
          clearTimeout(timeoutHandle);
          isListening = false;
          userCommandInProgress = false;
          detectionLocked = false;
        };

        r.onresult = e => {
          const t = e.results[0][0].transcript.trim().toLowerCase();
          if (t.includes("Ø±Ø¬ÙˆØ¹") || t.includes("Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©")) {
            speak("Ø¬Ø§Ø±ÙŠ Ø§Ù„Ø±Ø¬ÙˆØ¹ Ø¥Ù„Ù‰ Ø§Ù„ØµÙØ­Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©", () => {
              window.location.href = "/";
            });
          } else if (t.includes("Ù‡Ø¯Ù") || t.includes("Ø¨Ø­Ø«")) {
            window.location.reload();
          } else if (t.includes("Ø§Ø³ØªÙ…Ø±Ø§Ø±") || t.includes("ØªØ§Ø¨Ø¹")) {
            detectionCount = 0;
            cleanup();
            startDetection();
          } else {
            cleanup();
            startDetection();
          }
        };

        r.onerror = r.onend = () => {
          cleanup();
        };

        timeoutHandle = setTimeout(() => {
          console.warn("âŒ› Timeout: no response. Going back to homepage.");
          speak("Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø§Ø³ØªØ¬Ø§Ø¨Ø©ØŒ Ø³ÙŠØªÙ… Ø§Ù„Ø±Ø¬ÙˆØ¹ Ø¥Ù„Ù‰ Ø§Ù„ØµÙØ­Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©", () => {
            window.location.href = "/";
          });
        }, 15000); // 15 sec timeout

        beep.play();
        r.start();
        document.getElementById("heardText").innerText = "ğŸ¤ Ø§Ù„Ù…ÙŠÙƒØ±ÙˆÙÙˆÙ† ÙŠØ¹Ù…Ù„: Ø§Ø®ØªØ± Ø§Ù„Ø§Ø³ØªÙ…Ø±Ø§Ø± Ø£Ùˆ Ù‡Ø¯Ù Ø¢Ø®Ø± Ø£Ùˆ Ø§Ù„Ø±Ø¬ÙˆØ¹.";
      });
    }

    async function startDetection() {
      ws = new WebSocket(`ws://${location.host}/ws/depth-live`);

      ws.onopen = () => {
        ws.send(JSON.stringify({ target: detectionTarget }));
        console.log("âœ… WebSocket opened");
      };

      ws.onmessage = (event) => {
        if (detectionLocked) return;

        const data = JSON.parse(event.data);
        console.log("ğŸ“¦ WS Message Received:", JSON.stringify(data));

        if (!data || typeof data !== 'object' || !data.description) return;
        if (data.description === lastDescription) return;
        if (userCommandInProgress) return;

        lastDescription = data.description;
        document.getElementById('result').textContent = data.description;
        detectionCount++;

        const playAndCheck = () => {
          if (detectionCount >= 3) {
            askAfterDetections();
          }
        };

        if (data.audio_url) {
          fetch(data.audio_url)
            .then(response => response.blob())
            .then(blob => {
              const url = URL.createObjectURL(blob);
              if (audio.src !== url) {
                audio.pause();
                audio.currentTime = 0;
                audio.src = url;
              }
              audio.style.display = 'block';
              audio.play().then(() => {
                console.log("ğŸ”Š Audio playing...");
              }).catch(err => {
                console.warn("âŒ Audio play failed:", err.message);
              });
              audio.onended = playAndCheck;
            });
        }
      };

      ws.onclose = () => {
        console.warn("âŒ WebSocket closed");
      };

      const video = document.getElementById('video');
      const stream = await navigator.mediaDevices.getUserMedia({ video: true });
      video.srcObject = stream;

      const canvas = document.createElement('canvas');
      const ctx = canvas.getContext('2d');

      setInterval(() => {
        if (!detectionLocked && ws.readyState === 1) {
          canvas.width = video.videoWidth;
          canvas.height = video.videoHeight;
          ctx.drawImage(video, 0, 0);
          canvas.toBlob(blob => {
            const reader = new FileReader();
            reader.onload = () => {
              ws.send(reader.result);
            };
            reader.readAsDataURL(blob);
          }, 'image/jpeg', 0.7);
        }
      }, 500);
    }

    window.onload = () => {
      speak("Ù…Ø±Ø­Ø¨Ù‹Ø§ØŒ Ø§Ø®ØªØ± Ø§Ù„Ù‡Ø¯Ù Ø§Ù„Ù…Ø·Ù„ÙˆØ¨: 1 Ø´Ø®ØµØŒ 2 ÙƒØ±Ø³ÙŠØŒ 3 ÙƒÙ…Ø¨ÙŠÙˆØªØ±ØŒ 4 Ø­Ù‚ÙŠØ¨Ø©ØŒ 5 Ø·Ø§ÙˆÙ„Ø©ØŒ 6 ØªÙ„ÙØ§Ø². Ø£Ùˆ Ù‚Ù„ Ø±Ø¬ÙˆØ¹ Ù„Ù„Ø¹ÙˆØ¯Ø© Ø¥Ù„Ù‰ Ø§Ù„ØµÙØ­Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©.", () => {
        const r = new (window.SpeechRecognition || window.webkitSpeechRecognition)();
        r.lang = 'ar-SA';

        r.onresult = e => {
          const t = e.results[0][0].transcript.trim().toLowerCase();
          if (t.includes("Ø´Ø®Øµ") || t.includes("1")) detectionTarget = "person";
          else if (t.includes("ÙƒØ±Ø³ÙŠ") || t.includes("2")) detectionTarget = "chair";
          else if (t.includes("ÙƒÙ…Ø¨ÙŠÙˆØªØ±") || t.includes("3")) detectionTarget = "laptop";
          else if (t.includes("Ø­Ù‚ÙŠØ¨Ø©") || t.includes("4")) detectionTarget = "backpack";
          else if (t.includes("Ø·Ø§ÙˆÙ„Ø©") || t.includes("5")) detectionTarget = "dining table";
          else if (t.includes("ØªÙ„ÙØ§Ø²") || t.includes("6")) detectionTarget = "tv";
          else if (t.includes("Ø±Ø¬ÙˆØ¹") || t.includes("Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©")) {
            window.location.href = "/";
            return;
          }

          startDetection();
        };

        r.onerror = () => console.warn("ğŸ™ï¸ Initial SpeechRecognition error");
        r.start();
      });
    };
  </script>
<div class="bg"></div>
<div class="bg bg2"></div>
<div class="bg bg3"></div>
<div class="bg bg4"></div>

</body>
</html>
